{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient vs Evolution Strategies: Comparison on Wordle Task\n",
    "\n",
    "This notebook compares two optimization approaches on a Wordle prompt-engineering task:\n",
    "\n",
    "- **Policy Gradient (REINFORCE)**: First-order gradient-based method using Bernoulli policy\n",
    "- **Evolution Strategies (ES)**: Zeroth-order method using Gaussian perturbations\n",
    "\n",
    "Both methods optimize the selection of prompt modules (`format`, `track`, `info_gain`, `deduce`) to maximize reward on Wordle.\n",
    "\n",
    "**Experimental Setup:**\n",
    "- Small evaluation budget (5 eval calls per step, 8 total steps)\n",
    "- Fixed environment: `my-env` Wordle environment  \n",
    "- Fixed model: openai/gpt-4.1-nano\n",
    "- Fair comparison: ~40 total `prime_eval` calls for each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "\n",
      "Prompt modules: ['format', 'track', 'info_gain', 'deduce']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "# Import our implementations\n",
    "from src.pg import PGConfig, run_pg_step, BernoulliPromptPolicy\n",
    "from src.es import ESConfig, run_es_step, ContinuousPromptParams\n",
    "from src.policy_prompt import PROMPT_MODULES\n",
    "from src.utils_prime import find_eval_root, find_latest_results_file\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"\\nPrompt modules: {list(PROMPT_MODULES.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: my-env\n",
      "Model: openai/gpt-4.1-nano\n",
      "Steps: 2\n",
      "Eval per step: 5\n",
      "Total evals per method: ~10\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Shared experimental configuration\n",
    "ENV_ID = \"my-env\"\n",
    "MODEL = \"openai/gpt-4.1-nano\" \n",
    "N_STEPS = 2\n",
    "EVAL_PER_STEP = 5  # For fair comparison\n",
    "\n",
    "EVAL_KWARGS = {\n",
    "    \"n_episodes\": 3,\n",
    "    \"rollouts_per_example\": 1,\n",
    "    \"max_tokens\": 128,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "print(f\"Environment: {ENV_ID}\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Steps: {N_STEPS}\")\n",
    "print(f\"Eval per step: {EVAL_PER_STEP}\")\n",
    "print(f\"Total evals per method: ~{N_STEPS * EVAL_PER_STEP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Experiment\n",
    "\n",
    "REINFORCE uses a Bernoulli policy: each prompt module is included with probability $p_i = \\sigma(\\theta_i)$.\n",
    "\n",
    "Updates follow:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\cdot (r - b) \\cdot \\nabla \\log \\pi_\\theta(a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Gradient Configuration:\n",
      "  Learning rate: 0.3\n",
      "  Episodes per update: 5\n",
      "  Baseline: mean\n",
      "  Initial logits: {'format': 0.0, 'track': 0.0, 'info_gain': 0.0, 'deduce': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize PG policy with neutral logits\n",
    "pg_policy = BernoulliPromptPolicy(\n",
    "    logits={k: 0.0 for k in PROMPT_MODULES.keys()}\n",
    ")\n",
    "\n",
    "# PG configuration\n",
    "pg_config = PGConfig(\n",
    "    lr=0.3,\n",
    "    episodes_per_update=EVAL_PER_STEP,\n",
    "    baseline=\"mean\",\n",
    ")\n",
    "\n",
    "print(\"Policy Gradient Configuration:\")\n",
    "print(f\"  Learning rate: {pg_config.lr}\")\n",
    "print(f\"  Episodes per update: {pg_config.episodes_per_update}\")\n",
    "print(f\"  Baseline: {pg_config.baseline}\")\n",
    "print(f\"  Initial logits: {pg_policy.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Policy Gradient optimization...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Step 1/2:\n",
      "  Mean reward: 0.4200 ± 0.0000\n",
      "  Baseline: 0.4200\n",
      "  Logits: format=-0.00, track=-0.00, info_gain=0.00, deduce=-0.00\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run PG training\n",
    "pg_results = {\n",
    "    \"mean_rewards\": [],\n",
    "    \"std_rewards\": [],\n",
    "    \"logits_history\": [],\n",
    "}\n",
    "\n",
    "baseline_state = {}\n",
    "\n",
    "print(\"Running Policy Gradient optimization...\\n\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in range(N_STEPS):\n",
    "    pg_policy, info, baseline_state = run_pg_step(\n",
    "        policy=pg_policy,\n",
    "        cfg=pg_config,\n",
    "        env_id=ENV_ID,\n",
    "        model=MODEL,\n",
    "        eval_kwargs=EVAL_KWARGS,\n",
    "        baseline_state=baseline_state,\n",
    "    )\n",
    "    \n",
    "    rewards = info[\"rewards\"]\n",
    "    mean_r = np.mean(rewards)\n",
    "    std_r = np.std(rewards)\n",
    "    \n",
    "    pg_results[\"mean_rewards\"].append(mean_r)\n",
    "    pg_results[\"std_rewards\"].append(std_r)\n",
    "    pg_results[\"logits_history\"].append(dict(info[\"logits\"]))\n",
    "    \n",
    "    print(f\"Step {step + 1}/{N_STEPS}:\")\n",
    "    print(f\"  Mean reward: {mean_r:.4f} ± {std_r:.4f}\")\n",
    "    print(f\"  Baseline: {info['baseline']:.4f}\")\n",
    "    print(f\"  Logits: {', '.join([f'{k}={v:.2f}' for k, v in info['logits'].items()])}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n✓ Policy Gradient training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Strategies Experiment\n",
    "\n",
    "ES uses continuous weights and Gaussian perturbations to estimate gradients:\n",
    "\n",
    "$$\\nabla_\\theta \\mathbb{E}[R] \\approx \\frac{1}{N\\sigma} \\sum_{i=1}^{N} (r_i - b) \\epsilon_i$$\n",
    "\n",
    "where $\\epsilon_i \\sim \\mathcal{N}(0, I)$ and $r_i$ is the reward at $\\theta + \\sigma\\epsilon_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ES with neutral weights\n",
    "es_theta = ContinuousPromptParams(\n",
    "    weights={k: 0.0 for k in PROMPT_MODULES.keys()}\n",
    ")\n",
    "\n",
    "# ES configuration  \n",
    "es_config = ESConfig(\n",
    "    sigma=0.5,\n",
    "    lr=0.3,\n",
    "    population=EVAL_PER_STEP,\n",
    "    reward_baseline=\"mean\",\n",
    ")\n",
    "\n",
    "print(\"Evolution Strategies Configuration:\")\n",
    "print(f\"  Perturbation scale (σ): {es_config.sigma}\")\n",
    "print(f\"  Learning rate: {es_config.lr}\")\n",
    "print(f\"  Population size: {es_config.population}\")\n",
    "print(f\"  Baseline: {es_config.reward_baseline}\")\n",
    "print(f\"  Initial weights: {es_theta.weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ES training\n",
    "es_results = {\n",
    "    \"mean_rewards\": [],\n",
    "    \"std_rewards\": [],\n",
    "    \"weights_history\": [],\n",
    "}\n",
    "\n",
    "print(\"Running Evolution Strategies optimization...\\n\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in range(N_STEPS):\n",
    "    es_theta, info = run_es_step(\n",
    "        theta=es_theta,\n",
    "        cfg=es_config,\n",
    "        env_id=ENV_ID,\n",
    "        model=MODEL,\n",
    "        eval_kwargs=EVAL_KWARGS,\n",
    "    )\n",
    "    \n",
    "    rewards = info[\"rewards\"]\n",
    "    mean_r = np.mean(rewards)\n",
    "    std_r = np.std(rewards)\n",
    "    \n",
    "    es_results[\"mean_rewards\"].append(mean_r)\n",
    "    es_results[\"std_rewards\"].append(std_r)\n",
    "    es_results[\"weights_history\"].append(dict(info[\"new_weights\"]))\n",
    "    \n",
    "    print(f\"Step {step + 1}/{N_STEPS}:\")\n",
    "    print(f\"  Mean reward: {mean_r:.4f} ± {std_r:.4f}\")\n",
    "    print(f\"  Baseline: {info['baseline']:.4f}\")\n",
    "    print(f\"  Weights: {', '.join([f'{k}={v:.2f}' for k, v in info['new_weights'].items()])}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n✓ Evolution Strategies training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "steps = np.arange(1, N_STEPS + 1)\n",
    "\n",
    "# Plot PG results\n",
    "pg_means = np.array(pg_results[\"mean_rewards\"])\n",
    "pg_stds = np.array(pg_results[\"std_rewards\"])\n",
    "ax.plot(steps, pg_means, 'o-', label='Policy Gradient', linewidth=2, markersize=8)\n",
    "ax.fill_between(steps, pg_means - pg_stds, pg_means + pg_stds, alpha=0.2)\n",
    "\n",
    "# Plot ES results\n",
    "es_means = np.array(es_results[\"mean_rewards\"])\n",
    "es_stds = np.array(es_results[\"std_rewards\"])\n",
    "ax.plot(steps, es_means, 's-', label='Evolution Strategies', linewidth=2, markersize=8)\n",
    "ax.fill_between(steps, es_means - es_stds, es_means + es_stds, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Optimization Step', fontsize=12)\n",
    "ax.set_ylabel('Mean Reward', fontsize=12)\n",
    "ax.set_title('Policy Gradient vs Evolution Strategies on Wordle Task', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(steps)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Policy Gradient:\")\n",
    "print(f\"  Final mean reward: {pg_means[-1]:.4f}\")\n",
    "print(f\"  Reward improvement: {pg_means[-1] - pg_means[0]:.4f}\")\n",
    "print(f\"  Final logits: {', '.join([f'{k}={v:.2f}' for k, v in pg_results['logits_history'][-1].items()])}\")\n",
    "print()\n",
    "print(f\"Evolution Strategies:\")\n",
    "print(f\"  Final mean reward: {es_means[-1]:.4f}\")\n",
    "print(f\"  Reward improvement: {es_means[-1] - es_means[0]:.4f}\")\n",
    "print(f\"  Final weights: {', '.join([f'{k}={v:.2f}' for k, v in es_results['weights_history'][-1].items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Evolution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how parameters evolve over time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# PG logits evolution\n",
    "for key in PROMPT_MODULES.keys():\n",
    "    values = [hist[key] for hist in pg_results[\"logits_history\"]]\n",
    "    ax1.plot(steps, values, 'o-', label=key, linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Optimization Step', fontsize=11)\n",
    "ax1.set_ylabel('Logit Value', fontsize=11)\n",
    "ax1.set_title('Policy Gradient: Logit Evolution', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
    "ax1.set_xticks(steps)\n",
    "\n",
    "# ES weights evolution\n",
    "for key in PROMPT_MODULES.keys():\n",
    "    values = [hist[key] for hist in es_results[\"weights_history\"]]\n",
    "    ax2.plot(steps, values, 's-', label=key, linewidth=2, markersize=6)\n",
    "\n",
    "ax2.set_xlabel('Optimization Step', fontsize=11)\n",
    "ax2.set_ylabel('Weight Value', fontsize=11)\n",
    "ax2.set_title('Evolution Strategies: Weight Evolution', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
    "ax2.set_xticks(steps)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Convergence behavior**: Both methods should show some learning signal, though performance may vary due to the stochastic nature of the task and small sample size.\n",
    "\n",
    "2. **Parameter evolution**: \n",
    "   - PG logits directly represent log-odds of including each module\n",
    "   - ES weights represent continuous importance scores\n",
    "   - Both should show similar directional trends if modules are genuinely helpful\n",
    "\n",
    "3. **Variance**: ES may exhibit higher variance due to population-based sampling, while PG samples individual actions.\n",
    "\n",
    "4. **Computational cost**: Both methods use ~40 total evaluations, making them comparable in compute budget.\n",
    "\n",
    "**Limitations:**\n",
    "- Small sample size (8 steps × 5 evals) limits statistical power\n",
    "- Wordle is inherently high-variance  \n",
    "- Prompt engineering is a noisy optimization problem\n",
    "- No hyperparameter tuning performed\n",
    "\n",
    "**Next Steps:**\n",
    "- Run multiple seeds to assess consistency\n",
    "- Increase evaluation budget for stronger signal\n",
    "- Try different hyperparameters (learning rates, σ, population size)\n",
    "- Compare memory usage between methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
