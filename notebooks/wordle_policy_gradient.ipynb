{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wordle Policy Gradient Training with Qwen\n",
        "\n",
        "This notebook demonstrates training a Qwen language model using policy gradient methods (REINFORCE) to solve Wordle puzzles.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **Algorithm**: REINFORCE (Policy Gradient)\n",
        "- **Model**: Qwen2.5-0.5B (configurable)\n",
        "- **Environment**: Wordle game via Prime Intellect/TextArena\n",
        "- **Objective**: Learn to play Wordle through reinforcement learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.10.0+cu128\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add parent directory to path to import src modules\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from src.policy_gradient import PolicyGradientTrainer, TrainingConfig\n",
        "from src.wordle import load_environment\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure the training parameters. You can adjust these based on your compute resources and requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Configuration:\n",
            "  Model: Qwen/Qwen2.5-0.5B\n",
            "  Learning Rate: 1e-05\n",
            "  Batch Size: 4\n",
            "  Epochs: 5\n",
            "  Training Examples: 50\n",
            "  Eval Examples: 10\n",
            "  Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "config = TrainingConfig(\n",
        "    model_name=\"Qwen/Qwen2.5-0.5B\",  # Small model for testing - can use larger models\n",
        "    learning_rate=1e-5,              # Learning rate for optimizer\n",
        "    batch_size=4,                     # Batch size for training\n",
        "    num_epochs=5,                     # Number of training epochs\n",
        "    max_length=512,                   # Maximum sequence length\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    clip_grad_norm=1.0,              # Gradient clipping threshold\n",
        "    num_train_examples=50,            # Number of training examples\n",
        "    num_eval_examples=10,             # Number of evaluation examples\n",
        "    seed=42,                          # Random seed for reproducibility\n",
        ")\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Learning Rate: {config.learning_rate}\")\n",
        "print(f\"  Batch Size: {config.batch_size}\")\n",
        "print(f\"  Epochs: {config.num_epochs}\")\n",
        "print(f\"  Training Examples: {config.num_train_examples}\")\n",
        "print(f\"  Eval Examples: {config.num_eval_examples}\")\n",
        "print(f\"  Device: {config.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Trainer\n",
        "\n",
        "Create the policy gradient trainer with the Qwen model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-0.5B\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab66de8f924a429ba669f77618399202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea35328b78904dfca2f3c2a10089476f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de2d0916f5fb4510a5d330a1e52cc1aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb7da5341e85481db2fd1d6b2333b158",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a9b2141db6c40088829cf548aa97442",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-04 20:40:04 - verifiers.rubrics.rubric.RubricGroup - INFO - Initialized RubricGroup with 2 rubrics\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trainer initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer\n",
        "trainer = PolicyGradientTrainer(config)\n",
        "print(\"\\nTrainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Single Trajectory Generation\n",
        "\n",
        "Before training, let's test generating a single trajectory to see how the model interacts with the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing trajectory generation...\n"
          ]
        }
      ],
      "source": [
        "# Test generating a single trajectory\n",
        "print(\"Testing trajectory generation...\")\n",
        "trajectory, reward = trainer.generate_trajectory(example_idx=0)\n",
        "\n",
        "print(f\"\\nGenerated text (first 200 chars):\")\n",
        "print(trajectory[\"generated_text\"][:200])\n",
        "print(f\"\\nReward: {reward:.4f}\")\n",
        "print(f\"Log probabilities sum: {trajectory['log_probs_sum'].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Train the model using REINFORCE policy gradient algorithm. The model will learn to maximize rewards by adjusting its policy based on the rewards received."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on 50 examples\n",
            "Device: cpu\n",
            "\n",
            "Epoch 1/5\n",
            "Warning: Could not compute reward: 'RubricGroup' object has no attribute 'compute_reward'\n",
            "Warning: Could not compute reward: 'RubricGroup' object has no attribute 'compute_reward'\n"
          ]
        }
      ],
      "source": [
        "# Track training metrics\n",
        "training_losses = []\n",
        "training_rewards = []\n",
        "\n",
        "# Override train method to track metrics\n",
        "original_train = trainer.train\n",
        "\n",
        "def train_with_tracking():\n",
        "    \"\"\"Modified train method that tracks metrics.\"\"\"\n",
        "    print(f\"Starting training on {trainer.config.num_train_examples} examples\")\n",
        "    print(f\"Device: {trainer.device}\\n\")\n",
        "    \n",
        "    for epoch in range(trainer.config.num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{trainer.config.num_epochs}\")\n",
        "        \n",
        "        # Create batches\n",
        "        example_indices = list(range(trainer.config.num_train_examples))\n",
        "        np.random.shuffle(example_indices)\n",
        "        \n",
        "        epoch_losses = []\n",
        "        epoch_rewards = []\n",
        "        \n",
        "        # Process in batches\n",
        "        for i in range(0, len(example_indices), trainer.config.batch_size):\n",
        "            batch_indices = example_indices[i:i + trainer.config.batch_size]\n",
        "            metrics = trainer.train_step(batch_indices)\n",
        "            \n",
        "            epoch_losses.append(metrics[\"loss\"])\n",
        "            epoch_rewards.append(metrics[\"avg_reward\"])\n",
        "            \n",
        "            print(\n",
        "                f\"  Batch {i // trainer.config.batch_size + 1}: \"\n",
        "                f\"Loss={metrics['loss']:.4f}, \"\n",
        "                f\"Reward={metrics['avg_reward']:.4f} Â± {metrics['std_reward']:.4f}\"\n",
        "            )\n",
        "        \n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        avg_reward = np.mean(epoch_rewards)\n",
        "        \n",
        "        training_losses.append(avg_loss)\n",
        "        training_rewards.append(avg_reward)\n",
        "        \n",
        "        print(\n",
        "            f\"Epoch {epoch + 1} Summary: \"\n",
        "            f\"Avg Loss={avg_loss:.4f}, \"\n",
        "            f\"Avg Reward={avg_reward:.4f}\\n\"\n",
        "        )\n",
        "\n",
        "# Run training\n",
        "train_with_tracking()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress\n",
        "\n",
        "Plot the training loss and rewards over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training progress\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot loss\n",
        "ax1.plot(range(1, len(training_losses) + 1), training_losses, 'b-o')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot rewards\n",
        "ax2.plot(range(1, len(training_rewards) + 1), training_rewards, 'g-o')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Average Reward')\n",
        "ax2.set_title('Training Rewards')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final training loss: {training_losses[-1]:.4f}\")\n",
        "print(f\"Final training reward: {training_rewards[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluate the trained model on the evaluation set to measure its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "eval_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "for key, value in eval_metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Model on Sample Examples\n",
        "\n",
        "Let's see how the trained model performs on a few sample examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on a few examples\n",
        "num_test_examples = 5\n",
        "print(f\"Testing on {num_test_examples} examples:\\n\")\n",
        "\n",
        "for i in range(num_test_examples):\n",
        "    trajectory, reward = trainer.generate_trajectory(example_idx=i)\n",
        "    \n",
        "    print(f\"Example {i + 1}:\")\n",
        "    print(f\"  Reward: {reward:.4f}\")\n",
        "    print(f\"  Generated text preview: {trajectory['generated_text'][:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Information\n",
        "\n",
        "Display information about the model architecture and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model information\n",
        "total_params = sum(p.numel() for p in trainer.model.parameters())\n",
        "trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Model Information:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Model dtype: {next(trainer.model.parameters()).dtype}\")\n",
        "print(f\"  Device: {next(trainer.model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and Next Steps\n",
        "\n",
        "### What we've accomplished:\n",
        "- Implemented REINFORCE policy gradient algorithm\n",
        "- Trained a Qwen model on Wordle environment\n",
        "- Evaluated model performance\n",
        "\n",
        "### Potential improvements:\n",
        "- Try larger models (Qwen2.5-1.5B, Qwen2.5-3B)\n",
        "- Increase number of training examples\n",
        "- Adjust hyperparameters (learning rate, batch size)\n",
        "- Implement PPO (Proximal Policy Optimization) for more stable training\n",
        "- Add value function estimation (Actor-Critic methods)\n",
        "- Use reward shaping for better learning signal\n",
        "\n",
        "### Memory considerations:\n",
        "- Policy gradient methods require storing gradients, which can be memory-intensive\n",
        "- Consider using gradient checkpointing for larger models\n",
        "- Monitor GPU memory usage during training"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "wordle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
